{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64018cce-7611-45cb-b5fe-6b035095ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "def load_pkl_row(path):\n",
    "    try:\n",
    "        result = joblib.load(path)\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e)\n",
    "        print(\"Exception while loading:\", path)\n",
    "        result = {}\n",
    "    return result\n",
    "\n",
    "pandarallel.initialize()\n",
    "\n",
    "# load pkls and convert to dfs\n",
    "raw_df = pd.read_csv(\"all_ad_images.csv\", sep='\\t', header=0, index_col=0)\n",
    "# raw_df = pd.read_csv(\"all_ad_images_nima.csv\", sep='\\t', header=0, index_col=0)\n",
    "\n",
    "style_features_path = \"preprocessed/e4e/e4e_\"\n",
    "image_features_path = \"preprocessed/image_embedding/image_embedding_\"\n",
    "# text_features_bert_path = \"preprocessed/text_embedding/bert_\"\n",
    "\n",
    "style_features = raw_df.parallel_apply(lambda row: load_pkl_row(style_features_path + \n",
    "                                    row[\"image_name\"].replace(\".jpg\", \".pkl\")), axis=1)\n",
    "style_df = pd.DataFrame(style_features.tolist())\n",
    "gc.collect()\n",
    "del style_features\n",
    "gc.collect()\n",
    "print(\"style_features loading finished!\")\n",
    "\n",
    "image_features = raw_df.parallel_apply(lambda row: load_pkl_row(image_features_path + \n",
    "                                    row[\"image_name\"].replace(\".jpg\", \".pkl\")), axis=1)\n",
    "image_df = pd.DataFrame(image_features.tolist())\n",
    "gc.collect()\n",
    "del image_features\n",
    "gc.collect()\n",
    "print(\"image_features loading finished!\")\n",
    "\n",
    "# text_features_bert = raw_df.parallel_apply(lambda row: load_pkl_row(text_features_bert_path + \n",
    "#                                     row[\"image_name\"].replace(\".jpg\", \".pkl\")), axis=1)\n",
    "# text_bert_df = pd.DataFrame(text_features_bert.tolist())\n",
    "# gc.collect()\n",
    "# del text_features_bert\n",
    "# gc.collect()\n",
    "# print(\"text_features_bert loading finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11284a8-4226-4c75-80a4-5c5c4fb549d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import gc\n",
    "\n",
    "# raw_df = pd.read_csv(\"preprocessed/all_ad_images_nima.csv\", sep='\\t', header=0, index_col=0)\n",
    "# train_df = joblib.load(\"preprocessed/train_df_with_no_face.pkl\")\n",
    "\n",
    "# train_df[\"nima_mean\"] = raw_df[\"nima_mean\"]\n",
    "# train_df[\"nima_std\"] = raw_df[\"nima_std\"]\n",
    "# train_df = train_df[(train_df[\"nima_mean\"] != 0) | (train_df[\"nima_std\"] != 0)]\n",
    "# joblib.dump(train_df, \"preprocessed/train_df_with_no_face.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d47c-a978-4d7e-9d10-388d961b1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run when e4e is not finished, but only solo is finished.\n",
    "total image: \n",
    "Number of images with person: \n",
    "Ratio of person with Images: \n",
    "'''\n",
    "# solo_features_path = \"preprocessed/solo/solo_\"\n",
    "# solo_features = raw_df.parallel_apply(lambda row: joblib.load(solo_features_path + \n",
    "#                                     row[\"image_name\"].replace(\".jpg\", \".pkl\")), axis=1)\n",
    "# print(\"solo_features loading finished!\")\n",
    "# solo_df = pd.DataFrame(solo_features.tolist())\n",
    "\n",
    "# print(len(solo_df))\n",
    "# # images with at least one person:\n",
    "# num_person_images = (solo_df['person_count'] > 0).sum()\n",
    "# print(\"Number of images with person:\", num_person_images)\n",
    "# print(\"Ratio of person with Images:\", num_person_images / len(solo_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e72b5-ba84-4c9b-9bb2-ec7468f875d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some column names and count\n",
    "print(\"Number of rows should be same:\", len(raw_df), len(style_df), len(image_df))\n",
    "print(raw_df.columns)\n",
    "print(style_df.columns)\n",
    "print(image_df.columns)\n",
    "# print(text_bert_df.columns)\n",
    "\n",
    "# invalid images:\n",
    "num_invalid_images = (style_df['valid_image'] == False).sum()\n",
    "print(\"Number of invalid images:\", num_invalid_images)\n",
    "\n",
    "# # invalid texts:\n",
    "# num_invalid_texts = ((text_bert_df['valid_text'] == False)).sum()\n",
    "# print(\"Number of invalid texts:\", num_invalid_texts)\n",
    "\n",
    "# images with at least one person:\n",
    "num_person_images = (style_df['person_count'] > 0).sum()\n",
    "print(\"Number of images with person:\", num_person_images)\n",
    "print(\"Ratio of person with Images:\", num_person_images / len(style_df))\n",
    "\n",
    "# images with at least one face:\n",
    "num_face_images = (style_df['face_count'] > 0).sum()\n",
    "print(\"Number of images with face:\", num_face_images)\n",
    "print(\"Ratio of Images with face:\", num_face_images / len(style_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078d22f-c61f-43db-b879-52f476a80302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns\n",
    "raw_df = raw_df.rename({\"(total_click / total_show)\":'ctr'}, axis=1)\n",
    "# text_bert_df = text_bert_df.rename({'name_embedding':'name_embedding_bert'}, axis=1)\n",
    "\n",
    "# Merge dfs\n",
    "# text_bert_df[[\"name_embedding_bert\"]],\n",
    "merged_df = pd.concat(\n",
    "            [raw_df[[\"image_name\", \"product_name\",\n",
    "                     \"image_url\", \"total_click\", \"total_show\", \"ctr\"]],\n",
    "            style_df[[\"valid_image\", \"person_count\", \"person_masks\",\n",
    "                      \"category_labels\", \"category_name_labels\",\n",
    "                      \"face_count\", \"face_latents_index\", \"face_latents\"]],\n",
    "            image_df[[\"img_embedding\"]],\n",
    "        ], axis=1, join='inner')\n",
    "\n",
    "# Filter out rows with 0 ctr, because log(0) is -inf\n",
    "merged_df = merged_df[merged_df[\"ctr\"] != 0]\n",
    "# Filter out any row with na values\n",
    "merged_df = merged_df.dropna()\n",
    "# Filter out any rows with NIMA mean and std both equal zeros\n",
    "# merged_df = merged_df[(merged_df[\"nima_mean\"] != 0) | (merged_df[\"nima_std\"] != 0)]\n",
    "\n",
    "# Change img_embedding to np.arrays instead of list\n",
    "merged_df[\"img_embedding\"] = merged_df[\"img_embedding\"].apply(lambda x: np.array(x, dtype=np.float32))\n",
    "\n",
    "\n",
    "# CTR is left skewed, take log\n",
    "merged_df.insert(loc=6, column='log_ctr', value=np.log(merged_df[\"ctr\"]))\n",
    "# Standardize log CTR to overall mean and std\n",
    "s_log_ctr = (merged_df[\"log_ctr\"]-merged_df[\"log_ctr\"].mean())/merged_df[\"log_ctr\"].std()\n",
    "log_ctr_mean = merged_df[\"log_ctr\"].mean()\n",
    "log_ctr_std = merged_df[\"log_ctr\"].std()\n",
    "print(log_ctr_mean, log_ctr_std)\n",
    "merged_df.insert(loc=7, column='s_log_ctr', value=s_log_ctr)\n",
    "\n",
    "# Filter out invalid columns\n",
    "max_num_person = 5\n",
    "# filtered_df = merged_df[(merged_df[\"valid_image\"] == True) &\n",
    "#                 (merged_df['face_count'] > 0) & \n",
    "#                 (merged_df['face_count'] <= max_num_person)]\n",
    "filtered_df = merged_df[(merged_df[\"valid_image\"] == True) & \n",
    "                (merged_df['face_count'] <= max_num_person)]\n",
    "\n",
    "# Flatten face_latents, and pad to 5 person* 18*512..., to make it fixed length\n",
    "style_vector_dim = (18, 512)\n",
    "final_size = max_num_person * style_vector_dim[0] * style_vector_dim[1]\n",
    "\n",
    "# Flatten:\n",
    "filtered_df[\"face_latents\"] = filtered_df[\"face_latents\"].apply(lambda x : np.array(x, dtype=np.float32).flatten())\n",
    "# # Zero-Padding:\n",
    "# filtered_df[\"face_latents\"] = filtered_df[\"face_latents\"].apply(lambda x : \n",
    "#                 np.array(np.pad(x, (0, final_size - x.size), mode='constant', constant_values=0), dtype=np.float32))\n",
    "\n",
    "print(filtered_df[\"face_count\"].value_counts())\n",
    "\n",
    "# Prepare only the columns used for training and editing\n",
    "#                        'name_embedding_bert',\n",
    "train_df = filtered_df[[\"image_name\", \"product_name\", \"ctr\", \"log_ctr\", \"s_log_ctr\",\n",
    "                        \"category_labels\", \"category_name_labels\",\n",
    "                        \"face_count\", \"face_latents\", \n",
    "                        \"img_embedding\",\n",
    "                        \"face_latents_index\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ab1c8-6186-4e09-a821-2c64086dade1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "del raw_df\n",
    "del style_df\n",
    "del image_df\n",
    "# del text_bert_df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1735a1-40c5-435d-9408-525d69c32c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.columns)\n",
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5418d5c-fc1c-4f50-90a3-632b72501890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving full and filtered dfs\n",
    "# joblib.dump(train_df, \"preprocessed/train_df.pkl\")\n",
    "joblib.dump(train_df, \"preprocessed/train_df_with_no_face.pkl\")\n",
    "\n",
    "log_ctr_mean = merged_df[\"log_ctr\"].mean()\n",
    "log_ctr_std = merged_df[\"log_ctr\"].std()\n",
    "print(log_ctr_mean, log_ctr_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982dea2-7ce8-4052-a8d4-b5f2aebc3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Analysis\n",
    "# print(filtered_df[filtered_df[\"face_count\"] >0]['product_name'].value_counts())\n",
    "print(merged_df[\"face_count\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565116fc-f52a-40a6-8fe6-0f9abfe20233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(merged_df, \"preprocessed/merged_df.pkl\")\n",
    "joblib.dump(filtered_df, \"preprocessed/filtered_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16 (default, Jan 17 2023, 22:20:44) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "04af2e2c17f366a16105aa5e8a28d9fb6354f91c176a08f9c20b095b2be534b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
